{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Reinforcement Learning\n",
    "\n",
    "CREDIT: This notebook was inspired by [this Kaggle notebook on Q-learning](https:\/\/www.kaggle.com\/code\/kemquiros\/q-learning-using-btc-dataset\/notebook)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"d3f4MbxHCjxozXQxPb7Ufn",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"wQN7OT2Vnn8R9QdjwRncxP"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Imports"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"XxmOwyytHyZfl9Y7yxJtPI",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Jz1cvl45uclg4UORWmLTfO",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"5hBPcuqpi7UiT9dD43pjyw"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Dataset\n",
    "\n",
    "### Observe the dataset\n",
    "\n",
    "As usual, try to find out what you can about this dataset. [Here is the origin of the dataset](https:\/\/www.kaggle.com\/datasets\/billqi\/binance-bitcoin-futures-price-10s-intervals)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"2d7ZtTb2fNjr5Bgm8LQ12G",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"a8KMUOCx08TIgycg8xfCHs"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "prices = np.loadtxt('prices_btc_Jan_11_2020_to_May_22_2020.txt', dtype=float)"
   ],
   "execution_count":9,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"AAB62l1RkxiSoeBFuw3EWS",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"BTbZNWauwfvME6w0F2YAfh"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Reinforcement Learning\n",
    "\n",
    "We are going to define an environment from scratch. The objective is to take the right decisions in order to increase the amount of money you gain."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"VJC8CN5OdAqiHWckdwHewj",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"g2FO7NRvJO5bTqotePUcje"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Functions to buy, sell and wait\n",
    "\n",
    "Using the indications in the comments, implement the following functions. These will be the actions the agent can take."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"sC0Cy7VQ9Xykg9271R1Itz",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"w0O1Ug4iEWgJ9fjtLCx6e8"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def buy(btc_price, btc, money):\n",
    "    # if there is money\n",
    "    # spend all money to acquire bitcoin\n",
    "    return btc, money\n",
    "\n",
    "\n",
    "def sell(btc_price, btc, money):\n",
    "    # if there is bitcoin\n",
    "    # exchange all bitcoin for money\n",
    "    return btc, money\n",
    "\n",
    "\n",
    "def wait(btc_price, btc, money):\n",
    "    # do nothing\n",
    "    return btc, money"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"xJYKMXWPcN4dbdlwGkJjpW",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"T5CsKc7lg0IAIvO4JI3Y8e"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Create actions, states tables\n",
    "\n",
    "We now create the table of actions and states. A state will be the price of bitcoin."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"7y9cTmkBV29aNAWqLo8p3t",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"vPzKtOGMQdUh2gYZb3xvHO"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "np.random.seed(1)\n",
    "\n",
    "# set of actions that the user could do\n",
    "actions = { 'buy' : buy, 'sell': sell, 'wait' : wait}\n",
    "\n",
    "actions_to_number = { 'buy' : 0, 'sell' : 1, 'wait' : 2 }\n",
    "number_to_actions = { k:v for (k,v) in enumerate(actions_to_number) }\n",
    "\n",
    "number_actions = len(actions_to_number.keys())\n",
    "number_states = len(prices)\n",
    "\n",
    "# reference table for our agent to select the best action based on the q-value, initialized randomly\n",
    "q_table = np.random.rand(number_states, number_actions)"
   ],
   "execution_count":17,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"i4fJs7DrVKq3GQ9QVOQABD",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"jbcOrId1OYphZ0BhFJa2KK"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Functions to get rewards and act upon action"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"pcNdQfPOV8gx3u9yudYPX2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"OkDsKpTpEJnIpoR1SqYVGd"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_reward(before_btc, btc, before_money, money):\n",
    "    # if there is bitcoin, and if the amount of bitcoin increased\n",
    "    # the reward should be 1\n",
    "\n",
    "    # if there is money, and the amount of money increased\n",
    "    # the reward should also be 1\n",
    "\n",
    "    # if neither increased, the reward should be zero\n",
    "    return reward"
   ],
   "execution_count":18,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"hcYFG6321q7c6BuGf5EVGn",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"cUNnfyAGiVQohulMwVXhPz"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "In the following cell, implement the `choose_action` function with an epsilon-greedy policy. The function `choose action` returns a number indicating which action needs to be used (as a number)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"diImViV3Hgowu2ykbJ3rau",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def choose_action(state):\n",
    "    # with probability epsilon, choose a random action uniformly\n",
    "    # with probability 1-epsilon, choose the action that maximises q \n",
    "    return ..."
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"7KasW5wRCeiJKM8VyW81Ip",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"7MtP1TFzjBU6nXVvy8oUxq"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Now, implement a function that uses the tables above to execute the relevant action."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"WIptUweeZczFp3waG4B1gH",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def take_action(state, action):\n",
    "    return ..."
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zaXrqasfoJGCmFsGiVJ8m5",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"IXA1qqZdVsog4v7AUUG6Ti"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "In this next cell, we define a function that takes an action, updates the amounts of money, and returns the new state and reward."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"zj5AHCbIXAyc7vuVX8m6OH",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def act(state, action, theta):\n",
    "    btc, money = theta\n",
    "    \n",
    "    done = False\n",
    "    new_state = state + 1\n",
    "    \n",
    "    before_btc, before_money = btc, money\n",
    "    btc, money = take_action(state, action)\n",
    "    theta = btc, money\n",
    "    \n",
    "    reward = get_reward(before_btc, btc, before_money, money)\n",
    "    \n",
    "    if new_state == number_states:\n",
    "        done = True\n",
    "    \n",
    "    return new_state, reward, theta, done"
   ],
   "execution_count":19,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"pLxVFBvxdaHWgwT0S8dcmn",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"WMhlA12H1LOVQ10NVvqdk7"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Training the Q table"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"b8Y4tEZFA9CCtyWj7DKl8L",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"V1G4EJqBqWQUIT8D8G03nI"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "reward = 0\n",
    "btc = 0\n",
    "money = 100\n",
    "\n",
    "theta = btc, money"
   ],
   "execution_count":20,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"G1e0hQepD1UuFK3WzdT2qA",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"d47o8wHCYHpq7WVEA5D7OP"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# exploratory\n",
    "eps = 0.3\n",
    "\n",
    "n_episodes = 5\n",
    "min_alpha = 0.02\n",
    "\n",
    "# learning rate for Q learning\n",
    "alphas = np.linspace(1.0, min_alpha, n_episodes)\n",
    "\n",
    "# discount factor, used to balance immediate and future reward\n",
    "gamma = 1.0"
   ],
   "execution_count":21,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"bUfQRpvg0VsYj5WiZDd7lP",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"1ytoaqw1trOtlWYEpKF8yO"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "#### Steps for Q-network learning\n",
    "\n",
    "Here are the 3 basic steps:\n",
    "\n",
    "- Agent starts in a state=0 takes an action and receives a reward\n",
    "- Agent selects action by referencing Q-table with highest value (max) OR by random (epsilon, Îµ)\n",
    "- Update q-values\n",
    "\n",
    "Complete the cell underneath to update the q table (refer to the formula we saw in class!).\n",
    "\n",
    "The training process can be pretty long, so do not be alarmed!"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"nyJCeOi4ffVqM5KOLENYQk",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"4XJ4ZuKlTwqX4lcjUEE5xm"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "rewards = {}\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    state = 0\n",
    "    done = False\n",
    "    alpha = alphas[e]\n",
    "    \n",
    "    while(done != True):\n",
    "\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, theta, done = act(state, action, theta)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if(done):\n",
    "            rewards[e] = total_reward\n",
    "            print(f\"Episode {e + 1}: total reward -> {total_reward}\")\n",
    "            break\n",
    "        \n",
    "        q_table[state][action] = ... \n",
    "\n",
    "        state = next_state"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6cZSWhWzNAWHy7QbDAd8DS",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"BTYqIxuV9ijAPES514o6U0"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Learning Analysis\n",
    "\n",
    "In the cell underneath, plot the total reward as function of the episode. What do you observe?"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"2ej3TKGtE2b2zKGdWGmwBV",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"uBXJOTjggFfXmctuq47KYi"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Your code here"
   ],
   "execution_count":22,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"cVwrhRFTTviClSsbKun9BU",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"pD5Lja1RWJRaowMcEr9zX0"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Plot the Results\n",
    "\n",
    "The following cells let you visualize the decisions taken during the learning process.\n",
    "\n",
    "Find ways to interpret the results you obtained. Was your algorithm useful?"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"A11bYxcLtcnGO9IZRyKd1w",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"8G9t1CRqrSGmBHXetVJyO3"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "state = 0\n",
    "acts = np.zeros(number_states)\n",
    "done = False\n",
    "\n",
    "while(done != True):\n",
    "\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, theta, done = act(state, action, theta)\n",
    "        \n",
    "        acts[state] = action\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if(done):\n",
    "            break\n",
    "            \n",
    "        state = next_state"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"IUlngKUHrLfl4oT8HA0Ozl",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"WGEj5qOFRNJ6rpbzELS6AK"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "buys_idx = np.where(acts == 0)\n",
    "wait_idx = np.where(acts == 2)\n",
    "sell_idx = np.where(acts == 1)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"RoHSJbZnihfj8QnyeV4aJU",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"PkX3kT3rDxkLKxaehAZuHM"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(buys_idx[0], prices[buys_idx], 'bo', markersize=2)\n",
    "plt.plot(sell_idx[0], prices[sell_idx], 'ro', markersize=2)\n",
    "plt.plot(wait_idx[0], prices[wait_idx], 'yo', markersize=2)"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"YyMfs475OmGLBprvvfRPSl",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"JsKuD4AVtDHAKeTe8Qkqum"
     }
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    "wQN7OT2Vnn8R9QdjwRncxP",
    "uLyd4tGEZbyL6kUDH1zBxo",
    "7R2ikxFGtDyrhqhaA52mvv",
    "bekqRRGSR0visemPuU0HJ4",
    "5hBPcuqpi7UiT9dD43pjyw",
    "a8KMUOCx08TIgycg8xfCHs",
    "BTbZNWauwfvME6w0F2YAfh",
    "x4VL3goH1BMVuuwmYOKn4x",
    "csoVBxKbcd029XRhNSRiY5",
    "g2FO7NRvJO5bTqotePUcje",
    "w0O1Ug4iEWgJ9fjtLCx6e8",
    "T5CsKc7lg0IAIvO4JI3Y8e",
    "vPzKtOGMQdUh2gYZb3xvHO",
    "jbcOrId1OYphZ0BhFJa2KK",
    "OkDsKpTpEJnIpoR1SqYVGd",
    "cUNnfyAGiVQohulMwVXhPz",
    "7MtP1TFzjBU6nXVvy8oUxq",
    "IXA1qqZdVsog4v7AUUG6Ti",
    "WMhlA12H1LOVQ10NVvqdk7",
    "V1G4EJqBqWQUIT8D8G03nI",
    "d47o8wHCYHpq7WVEA5D7OP",
    "1ytoaqw1trOtlWYEpKF8yO",
    "4XJ4ZuKlTwqX4lcjUEE5xm",
    "BTYqIxuV9ijAPES514o6U0",
    "uBXJOTjggFfXmctuq47KYi",
    "pD5Lja1RWJRaowMcEr9zX0",
    "8G9t1CRqrSGmBHXetVJyO3",
    "WGEj5qOFRNJ6rpbzELS6AK",
    "PkX3kT3rDxkLKxaehAZuHM",
    "JsKuD4AVtDHAKeTe8Qkqum",
    "TNINYuxoQG3zSJv8mmRNCa"
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}