{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Working with text\n",
    "\n",
    "CREDIT: This practical was inspired from [this notebook on NLP](https://www.kaggle.com/code/amar09/text-pre-processing-and-feature-extraction)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd                                     # for dataset manipulation (DataFrames)\n",
    "import numpy as np                                      # allows some mathematical operations\n",
    "import matplotlib.pyplot as plt                         # library used to display graphs\n",
    "import seaborn as sns                                   # more convenient visualisation library for dataframes\n",
    "from sklearn.model_selection import train_test_split    # for classification\n",
    "from sklearn.svm import LinearSVC                       # for classification\n",
    "from sklearn.metrics import confusion_matrix            # for classification\n",
    "from sklearn.metrics import accuracy_score              # for classification\n",
    "import imblearn                                         # for imbalance management\n",
    "import time                                             # for execution time measurement\n",
    "import nltk                                             # Natural Language ToolKit for NLP"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T12:03:39.965501Z",
     "start_time": "2023-08-04T12:03:39.258492Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Today's dataset is the [IMDB Movie Reviews Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb_dataset.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T11:58:33.619031Z",
     "start_time": "2023-08-04T11:58:33.084758Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using what you have learned in the previous lessons, examine the datasets and see what you can learn about them.\n",
    "In particular, identify the classification task this dataset was created for, and the potential issues you could encounter.\n",
    "Are the classes balanced?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T12:04:36.161822Z",
     "start_time": "2023-08-04T12:04:36.160638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysing the reviews\n",
    "\n",
    "In order to see what needs to be cleaned, let us first observe the most common words in the dataset.\n",
    "\n",
    "1. Create a function `create_corpus(texts)` that takes a list / pd.Series of strings, and outputs a list of all the individual words contained in it.\n",
    "*Hint: You may need to use the `str.split` function.*\n",
    "2. Display the most common words in the IMDB dataset.\n",
    "3. Comment on your observations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def create_corpus(texts):\n",
    "    corpus = []\n",
    "    ... # Your code here\n",
    "    return pd.DataFrame(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T12:23:06.575505Z",
     "start_time": "2023-08-04T12:23:06.569205Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The NLTK package offers a list of stopwords, which are common words in a language that carry little to no meaning.\n",
    "Display the most common words in the dataset, this time ignoring stop words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T12:38:55.927038Z",
     "start_time": "2023-08-04T12:38:55.915928Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning the data\n",
    "### Removal of stop words\n",
    "\n",
    "1. Using the list of stopwords downloaded above, implement a `remove_stopwords(text)` that takes a string as input, and outputs the same string where stopwords are removed.\n",
    "2. Apply this function to the data.\n",
    "\n",
    "*Hint: You can do it on your own, or you can look into `str.translate` and `str.maketrans`.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def remove_stopwords(text:str):\n",
    "    ... # Your code here\n",
    "    return ..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T13:03:26.351099Z",
     "start_time": "2023-08-04T13:03:26.340814Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removal of punctuation\n",
    "\n",
    "1. Using the native `string.punctuation` list, implement a `remove_punctuation(text)` that takes a string as input, and outputs the same string where punctuation is removed.\n",
    "2. Apply this function to the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_list = string.punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str):\n",
    "    ...  # Your code here\n",
    "    return ..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T13:05:16.474081Z",
     "start_time": "2023-08-04T13:05:16.472185Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "1. What are stemming and lemmatization? What are some differences between the two?\n",
    "2. Implement a `stem_text(text)` function that takes a string as input and outputs the same string where words have been stemmed using NLTK's `PorterStemmer`.\n",
    "3. Implement a `lemmatize_text(text)` function that takes a string as input and outputs the same string where words have been lemmatized using NLTK's `WorldNetLemmatizer`.\n",
    "4. Apply stemming and lemmatization to the dataset and store the results in two different columns. Compare and comment on the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    ...  # Your code here\n",
    "    return ...\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ...  # Your code here\n",
    "    return ..."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T13:24:29.174467Z",
     "start_time": "2023-08-04T13:24:29.168498Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bonus: Other types of pre-processing\n",
    "\n",
    "We could have done many other types of cleaning: removing emojis, removing URLs, spellchecking, removing frequent or rare words, etc.\n",
    "If you want to try performing these cleaning steps yourself, feel free to refer to [this kaggle notebook](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T13:24:58.994723Z",
     "start_time": "2023-08-04T13:24:58.983420Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*[Your comments here]*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
